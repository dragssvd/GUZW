{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Laboratorium 7\n\nCelem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"},{"metadata":{},"cell_type":"markdown","source":"Dołączenie standardowych bibliotek"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import deque\nimport gym\nimport numpy as np\nimport random","execution_count":1,"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'gym'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7a828ff9d16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"]}]},{"metadata":{},"cell_type":"markdown","source":"Dołączenie bibliotek do obsługi sieci neuronowych"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Zadanie 1 - Actor-Critic\n\n<p style='text-align: justify;'>\nCelem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\nWagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n\\begin{equation*}\n    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n\\end{equation*}\nWagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n\\begin{equation*}\n    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n\\end{equation*}\ngdzie:\n\\begin{equation*}\n    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n\\end{equation*}\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class REINFORCEAgent:\n    def __init__(self, state_size, action_size, actor, critic):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99    # discount rate\n        self.learning_rate = 0.001\n        self.actor = actor\n        self.critic = critic #critic network should have only one output\n\n\n    def get_action(self, state):\n        \"\"\"\n        Compute the action to take in the current state, basing on policy returned by the network.\n\n        Note: To pick action according to the probability generated by the network\n        \"\"\"\n\n        #\n        # INSERT CODE HERE to get action in a given state\n        #        \n        \n        return chosen_action\n\n  \n\n    def learn(self, state, action, reward, next_state, done):\n        \"\"\"\n        Function learn networks using information about state, action, reward and next state. \n        First the values for state and next_state should be estimated based on output of critic network.\n        Critic network should be trained based on target value:\n        target = r + \\gamma next_state_value if not done]\n        target = r if done.\n        Actor network shpuld be trained based on delta value:\n        delta = target - state_value\n        \"\"\"\n        #\n        # INSERT CODE HERE to train network\n        #\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"CartPole-v0\").env\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nalpha_learning_rate = 0.0001\nbeta_learning_rate = 0.0005\n\nactor_model =         \n        #\n        # INSERT CODE HERE to build actor network, in the last layer use softmax activation function\n        #\n\ncritic_model =         \n        #\n        # INSERT CODE HERE to build critic network, in the last layer use linear activation function, network should have single output\n        #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas nauczyć agenta gry w środowisku *CartPool*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = Agent(state_size, action_size, actor_model, critic_model)\n\n\nfor i in range(100):\n    score_history = []\n\n    for i in range(100):\n        done = False\n        score = 0\n        state = env.reset()\n        while not done:\n            action = agent.choose_action(state)\n            next_state, reward, done, _ = env.step(action)\n            agent.learn(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n        score_history.append(score)\n\n    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n\n    if np.mean(score_history) > 300:\n        print(\"You Win!\")\n        break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}