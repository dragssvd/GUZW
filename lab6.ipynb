{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 6\n",
    "\n",
    "Celem szóstego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - REINFORCE. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import ndarray\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    return np.array([np.sum([rewards[j] * gamma**(j - i) for j in range(i, len(rewards))]) for i in range(len(rewards))])\n",
    "\n",
    "\n",
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - REINFORCE\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu REINFORCE. Wagi sieci aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha G_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta)\n",
    "\\end{equation*}.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, lr, state_shape, n_actions, fc1, fc2):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_shape, fc1)\n",
    "        self.fc2 = nn.Linear(fc1, fc2)\n",
    "        self.output = nn.Linear(fc2, n_actions)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device('cuda:0') if T.cuda.is_available() else T.device('cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        probs = self.softmax(self.output(state))\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_shape, n_actions, fc1: int = 128, fc2: int = 128, lr: float = 0.001, gamma: float = 0.99):\n",
    "        self.action_size = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.model = PGN(lr, state_shape, n_actions, fc1, fc2)\n",
    "        self.action_log_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "    def store_transition(self, action_log, reward):\n",
    "        self.action_log_memory.append(action_log)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = T.tensor(state).to(self.model.device)\n",
    "        probs = self.model.forward(state)\n",
    "        cat = Categorical(probs)\n",
    "        action = cat.sample()\n",
    "\n",
    "        return action.item(), cat.log_prob(action)\n",
    "\n",
    "    def learn(self):\n",
    "        G = get_cumulative_rewards(self.reward_memory, self.gamma)\n",
    "        rewards = T.tensor(G).to(self.model.device)\n",
    "        log_probs = T.stack(self.action_log_memory)\n",
    "\n",
    "        loss = -T.mean(rewards * log_probs)\n",
    "        self.model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model.optimizer.step()\n",
    "\n",
    "        self.action_log_memory = []\n",
    "        self.reward_memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_shape = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "learning_rate = 0.001\n",
    "\n",
    "np.bool8 = np.bool_\n",
    "agent = REINFORCEAgent(state_shape=state_shape, n_actions=n_actions, lr=learning_rate, gamma=0.99, fc1=128, fc2=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotuj funkcję obliczającą wartość nagrody skumulowanej:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:22.320\n",
      "mean reward:37.040\n",
      "mean reward:58.470\n",
      "mean reward:142.790\n",
      "mean reward:219.510\n",
      "mean reward:291.090\n",
      "mean reward:115.850\n",
      "mean reward:93.040\n",
      "mean reward:144.320\n",
      "mean reward:272.700\n",
      "mean reward:1000.000\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state[0].astype(np.float32)\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # chose action\n",
    "        action, action_log = agent.choose_action(state)\n",
    "\n",
    "        _state, reward, done, _, info = env.step(action)\n",
    "        _state = _state.astype(np.float32)\n",
    "\n",
    "        # record session history to train later\n",
    "        agent.store_transition(action_log, reward)\n",
    "\n",
    "        reward += reward\n",
    "\n",
    "        state = _state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.learn()\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
